{
  "llama": [
    {
      "id": 1,
      "name": "GPT4All",
      "domain": "general",
      "general_avg": 27.88,
      "math_avg": 13.79,
      "code_avg": 34.76,
      "reasoning_avg": 8.02,
      "overall_avg": 21.11,
      "general_scores": [
        45.88,
        30.76,
        29.51,
        5.28642857,
        42.43,
        34.1625,
        29.15,
        6.52857143,
        43.37,
        31.745,
        30.01,
        5.76714286
      ],
      "math_scores": [
        43.06,
        9.2,
        11.0,
        7.66,
        3.33,
        36.54,
        9.76,
        10.4,
        7.7,
        0.0,
        42.53,
        8.78,
        9.6,
        7.36,
        0.0
      ],
      "code_scores": [
        32.32,
        45.53,
        4.3,
        27.56,
        9.73,
        66.44,
        49.22,
        39.02,
        50.58,
        1.79,
        29.23,
        17.65,
        61.69,
        46.13,
        35.98,
        49.03,
        3.58,
        29.85,
        18.55,
        63.39,
        48.34
      ],
      "reasoning_scores": [
        0.28815217,
        13.68,
        0.29815217,
        18.24,
        0.31217391,
        15.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.89
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.22
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.56
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.86
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.71
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.25
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 35.77
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.22
              },
              {
                "metric": "lcb_code_execution",
                "score": 28.88
              },
              {
                "metric": "lcb_test_output",
                "score": 15.31
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 63.84
              },
              {
                "metric": "hard instruct",
                "score": 47.9
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 15.73
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 2,
      "name": "Alpaca",
      "domain": "general",
      "general_avg": 34.07,
      "math_avg": 4.07,
      "code_avg": 32.83,
      "reasoning_avg": 10.33,
      "overall_avg": 20.32,
      "general_scores": [
        38.3,
        42.8125,
        33.56,
        16.8542857,
        48.65,
        44.035,
        35.03,
        12.1478571,
        49.07,
        36.945,
        35.66,
        15.7421429
      ],
      "math_scores": [
        1.82,
        4.62,
        4.6,
        6.05,
        0.0,
        2.58,
        4.74,
        4.8,
        6.01,
        3.33,
        4.17,
        4.64,
        4.0,
        6.35,
        3.33
      ],
      "code_scores": [
        32.32,
        45.91,
        3.23,
        24.84,
        5.43,
        69.83,
        37.81,
        30.49,
        50.19,
        2.51,
        27.56,
        18.1,
        70.85,
        43.84,
        29.27,
        48.64,
        3.94,
        28.18,
        9.05,
        69.15,
        38.37
      ],
      "reasoning_scores": [
        0.3026087,
        20.0,
        0.29945652,
        20.64,
        0.30956522,
        20.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.34
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.26
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.75
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.91
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.86
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.67
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.14
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 30.69
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.25
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.86
              },
              {
                "metric": "lcb_test_output",
                "score": 10.86
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 69.94
              },
              {
                "metric": "hard instruct",
                "score": 40.01
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.35
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 3,
      "name": "Alpaca_GPT4",
      "domain": "general",
      "general_avg": 32.52,
      "math_avg": 7.57,
      "code_avg": 33.2,
      "reasoning_avg": 11.55,
      "overall_avg": 21.21,
      "general_scores": [
        25.39,
        41.765,
        33.13,
        12.5321429,
        47.94,
        41.5825,
        33.19,
        13.2785714,
        50.1,
        42.3275,
        34.28,
        14.7607143
      ],
      "math_scores": [
        18.04,
        8.54,
        8.8,
        9.35,
        0.0,
        14.86,
        7.06,
        7.4,
        9.98,
        0.0,
        8.72,
        6.76,
        6.0,
        8.06,
        0.0
      ],
      "code_scores": [
        37.2,
        52.14,
        2.51,
        23.8,
        20.36,
        54.58,
        37.05,
        41.46,
        53.7,
        3.58,
        21.92,
        20.36,
        55.93,
        29.86,
        42.68,
        56.81,
        6.09,
        21.29,
        20.36,
        56.27,
        39.27
      ],
      "reasoning_scores": [
        0.32521739,
        23.36,
        0.3526087,
        22.0,
        0.32434783,
        22.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.89
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.53
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.52
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.87
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.45
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.13
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 40.45
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.22
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.06
              },
              {
                "metric": "lcb_code_execution",
                "score": 22.34
              },
              {
                "metric": "lcb_test_output",
                "score": 20.36
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 55.59
              },
              {
                "metric": "hard instruct",
                "score": 35.39
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 22.77
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 4,
      "name": "WizardLM_evol_instruct_V2_196k (实际只有143k)",
      "domain": "general",
      "general_avg": 38.04,
      "math_avg": 15.47,
      "code_avg": 38.14,
      "reasoning_avg": 12.82,
      "overall_avg": 26.12,
      "general_scores": [
        41.16,
        47.9275,
        35.71,
        26.5814286,
        49.22,
        49.1075,
        33.58,
        20.6507143,
        45.08,
        49.3525,
        34.68,
        23.4607143
      ],
      "math_scores": [
        45.11,
        12.7,
        13.8,
        8.11,
        3.33,
        41.09,
        12.92,
        11.8,
        7.72,
        0.0,
        40.41,
        12.96,
        14.4,
        7.7,
        0.0
      ],
      "code_scores": [
        43.9,
        55.64,
        5.02,
        12.32,
        18.33,
        71.86,
        54.41,
        50.61,
        52.53,
        3.23,
        18.16,
        17.42,
        69.83,
        55.51,
        45.12,
        57.59,
        5.73,
        22.34,
        14.48,
        70.51,
        56.47
      ],
      "reasoning_scores": [
        0.31336957,
        24.72,
        0.35206522,
        25.28,
        0.33565217,
        25.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.15
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.8
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.66
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.56
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.2
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.86
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.84
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.54
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.25
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 17.61
              },
              {
                "metric": "lcb_test_output",
                "score": 16.74
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 70.73
              },
              {
                "metric": "hard instruct",
                "score": 55.46
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.33
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.31
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 5,
      "name": "LIMA",
      "domain": "general",
      "general_avg": 30.01,
      "math_avg": 5.61,
      "code_avg": 31.64,
      "reasoning_avg": 2.86,
      "overall_avg": 17.53,
      "general_scores": [
        66.09,
        29.2525,
        24.29,
        0.24857143,
        66.03,
        30.36,
        23.85,
        0.17142857,
        65.83,
        29.9575,
        23.84,
        0.23357143
      ],
      "math_scores": [
        7.73,
        5.6,
        4.0,
        10.52,
        0.0,
        7.73,
        5.6,
        4.6,
        10.46,
        0.0,
        7.51,
        5.76,
        4.2,
        10.48,
        0.0
      ],
      "code_scores": [
        40.24,
        59.14,
        0.36,
        17.95,
        0.45,
        43.05,
        61.43,
        40.85,
        58.75,
        0.36,
        18.16,
        0.23,
        42.37,
        61.26,
        38.41,
        58.37,
        0.36,
        19.21,
        0.23,
        42.03,
        61.3
      ],
      "reasoning_scores": [
        0.27391304,
        5.68,
        0.27641304,
        5.6,
        0.27717391,
        5.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.98
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 29.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.99
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.22
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.66
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 39.83
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 58.75
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.36
              },
              {
                "metric": "lcb_code_execution",
                "score": 18.44
              },
              {
                "metric": "lcb_test_output",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 42.48
              },
              {
                "metric": "hard instruct",
                "score": 61.33
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 5.44
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 6,
      "name": "microsoft/orca-agentinstruct-1M-v1",
      "domain": "general",
      "general_avg": 47.35,
      "math_avg": 24.72,
      "code_avg": 40.88,
      "reasoning_avg": 15.97,
      "overall_avg": 32.23,
      "general_scores": [
        70.08,
        54.355,
        37.42,
        28.1542857,
        69.38,
        55.04,
        39.08,
        28.6621429,
        67.95,
        54.7825,
        38.46,
        24.82
      ],
      "math_scores": [
        47.84,
        30.1,
        34.2,
        13.37,
        0.0,
        51.78,
        31.4,
        30.6,
        14.23,
        0.0,
        47.46,
        28.44,
        28.6,
        12.76,
        0.0
      ],
      "code_scores": [
        45.73,
        48.64,
        7.53,
        18.37,
        29.64,
        75.93,
        62.69,
        45.73,
        50.58,
        6.45,
        13.15,
        26.7,
        77.97,
        60.88,
        40.85,
        49.42,
        6.81,
        29.44,
        24.89,
        74.92,
        62.07
      ],
      "reasoning_scores": [
        0.34293478,
        31.28,
        0.36043478,
        31.28,
        0.3451087,
        32.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 69.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 38.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.03
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.98
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.45
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 44.1
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 49.55
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.93
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.32
              },
              {
                "metric": "lcb_test_output",
                "score": 27.08
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 76.27
              },
              {
                "metric": "hard instruct",
                "score": 61.88
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.35
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.6
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 7,
      "name": "OpenHermes-2.5",
      "domain": "general",
      "general_avg": 44.34,
      "math_avg": 20.21,
      "code_avg": 40.63,
      "reasoning_avg": 7.44,
      "overall_avg": 28.15,
      "general_scores": [
        64.06,
        50.9275,
        39.9,
        25.0585714,
        58.97,
        52.215,
        39.98,
        24.3621429,
        60.95,
        50.3825,
        39.81,
        25.4792857
      ],
      "math_scores": [
        60.2,
        17.3,
        19.8,
        11.72,
        0.0,
        60.5,
        13.56,
        11.0,
        10.28,
        0.0,
        60.12,
        14.62,
        14.4,
        9.64,
        0.0
      ],
      "code_scores": [
        46.95,
        52.53,
        6.09,
        29.65,
        15.16,
        76.61,
        54.92,
        49.39,
        52.53,
        3.58,
        27.56,
        16.29,
        79.32,
        54.08,
        50.0,
        56.42,
        4.66,
        27.56,
        16.29,
        78.98,
        54.66
      ],
      "reasoning_scores": [
        0.38608696,
        15.52,
        0.3676087,
        17.76,
        0.34847826,
        10.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.18
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.9
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.97
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.27
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.55
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 48.78
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.83
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.78
              },
              {
                "metric": "lcb_code_execution",
                "score": 28.26
              },
              {
                "metric": "lcb_test_output",
                "score": 15.91
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 78.3
              },
              {
                "metric": "hard instruct",
                "score": 54.55
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 14.51
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 8,
      "name": "SelfInstruct",
      "domain": "general",
      "general_avg": 9.56,
      "math_avg": 0.72,
      "code_avg": 9.05,
      "reasoning_avg": 3.32,
      "overall_avg": 5.66,
      "general_scores": [
        17.68,
        3.62,
        7.94071429,
        5.56,
        8.61,
        12.9621429,
        17.73,
        3.9,
        8.04428571
      ],
      "math_scores": [
        0.61,
        0.92,
        0.6,
        0.0,
        0.38,
        1.84,
        1.6,
        0.0,
        0.38,
        1.36,
        1.0,
        0.0
      ],
      "code_scores": [
        7.93,
        14.79,
        0.0,
        3.76,
        0.0,
        12.2,
        22.01,
        9.15,
        19.46,
        0.0,
        0.21,
        0.0,
        15.59,
        22.93,
        6.1,
        17.12,
        0.0,
        0.0,
        0.0,
        22.71,
        16.11
      ],
      "reasoning_scores": [
        0.25108696,
        7.68,
        0.25021739,
        7.36,
        0.23391304,
        4.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.66
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 5.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 9.65
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.46
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.37
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 7.73
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 17.12
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 1.32
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 16.83
              },
              {
                "metric": "hard instruct",
                "score": 20.35
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 6.4
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 9,
      "name": "AutoCoT",
      "domain": "general",
      "general_avg": 28.38,
      "math_avg": 3.6,
      "code_avg": 19.83,
      "reasoning_avg": 1.76,
      "overall_avg": 13.39,
      "general_scores": [
        67.91,
        14.0225,
        30.84,
        0.02928571,
        67.22,
        14.905,
        32.86,
        0.00857143,
        67.01,
        13.04,
        32.64,
        0.05071429
      ],
      "math_scores": [
        9.86,
        4.52,
        1.8,
        6.71,
        0.0,
        2.43,
        2.68,
        3.4,
        6.82,
        0.0,
        3.94,
        3.1,
        2.2,
        6.53,
        0.0
      ],
      "code_scores": [
        0.0,
        49.03,
        0.0,
        1.25,
        0.0,
        65.08,
        40.3,
        0.0,
        9.73,
        0.0,
        0.21,
        0.0,
        66.44,
        37.15,
        0.0,
        41.25,
        0.0,
        0.42,
        0.0,
        64.41,
        41.17
      ],
      "reasoning_scores": [
        0.22956522,
        3.12,
        0.24478261,
        3.12,
        0.24021739,
        3.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 67.38
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.11
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.03
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 5.41
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.43
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.69
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 33.34
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.63
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 65.31
              },
              {
                "metric": "hard instruct",
                "score": 39.54
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.24
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 3.28
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 10,
      "name": "Orca-Math",
      "domain": "math",
      "general_avg": 35.08,
      "math_avg": 4.32,
      "code_avg": 23.22,
      "reasoning_avg": 7.94,
      "overall_avg": 17.64,
      "general_scores": [
        71.93,
        31.235,
        31.68,
        2.60785714,
        73.08,
        32.4475,
        32.67,
        3.575,
        73.01,
        34.6325,
        31.73,
        2.355
      ],
      "math_scores": [
        2.65,
        5.2,
        5.4,
        10.7,
        3.33,
        0.53,
        3.1,
        3.0,
        11.86,
        0.0,
        0.76,
        3.58,
        3.2,
        11.43,
        0.0
      ],
      "code_scores": [
        22.56,
        38.13,
        2.15,
        2.71,
        13.12,
        32.54,
        41.37,
        27.44,
        40.47,
        2.51,
        6.05,
        10.86,
        37.97,
        45.11,
        23.17,
        39.3,
        0.72,
        8.98,
        15.16,
        36.27,
        41.01
      ],
      "reasoning_scores": [
        0.21934783,
        14.48,
        0.2176087,
        14.64,
        0.22934783,
        17.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.77
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.03
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.31
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.96
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 24.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 39.3
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.79
              },
              {
                "metric": "lcb_code_execution",
                "score": 5.91
              },
              {
                "metric": "lcb_test_output",
                "score": 13.05
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 35.59
              },
              {
                "metric": "hard instruct",
                "score": 42.5
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 15.65
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 11,
      "name": "camel-ai/math",
      "domain": "math",
      "general_avg": 30.01,
      "math_avg": 17.26,
      "code_avg": 22.78,
      "reasoning_avg": 3.69,
      "overall_avg": 18.44,
      "general_scores": [
        71.36,
        21.0,
        25.47,
        0.02071429,
        72.35,
        24.7925,
        25.27,
        0.08857143,
        72.93,
        21.8575,
        24.91,
        0.02214286
      ],
      "math_scores": [
        40.86,
        17.18,
        18.4,
        8.79,
        0.0,
        41.24,
        17.9,
        17.6,
        9.55,
        0.0,
        38.67,
        18.1,
        18.4,
        8.9,
        3.33
      ],
      "code_scores": [
        0.0,
        54.09,
        0.36,
        5.22,
        0.9,
        48.14,
        54.08,
        1.22,
        55.25,
        1.43,
        3.34,
        2.26,
        37.63,
        55.61,
        1.22,
        56.81,
        1.08,
        5.01,
        1.36,
        41.36,
        52.05
      ],
      "reasoning_scores": [
        0.16869565,
        3.84,
        0.16869565,
        12.08,
        0.15163043,
        5.76
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 22.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.22
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.04
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 40.26
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.73
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.08
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 0.81
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.96
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.52
              },
              {
                "metric": "lcb_test_output",
                "score": 1.51
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 42.38
              },
              {
                "metric": "hard instruct",
                "score": 53.91
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.16
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 7.23
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 12,
      "name": "SkunkworksAI/reasoning-0.01",
      "domain": "math",
      "general_avg": 32.93,
      "math_avg": 10.81,
      "code_avg": 22.86,
      "reasoning_avg": 10.05,
      "overall_avg": 19.16,
      "general_scores": [
        55.66,
        33.68,
        35.43,
        6.08071429,
        58.42,
        32.125,
        36.03,
        3.90071429,
        61.88,
        31.85,
        35.99,
        4.13571429
      ],
      "math_scores": [
        21.3,
        11.1,
        10.8,
        11.36,
        0.0,
        21.68,
        12.36,
        9.8,
        11.36,
        0.0,
        19.94,
        11.28,
        10.2,
        11.0,
        0.0
      ],
      "code_scores": [
        8.54,
        55.64,
        2.51,
        3.13,
        1.36,
        55.25,
        39.81,
        2.44,
        52.14,
        2.87,
        4.59,
        2.26,
        53.22,
        39.74,
        1.22,
        54.47,
        2.51,
        4.38,
        1.58,
        53.56,
        38.89
      ],
      "reasoning_scores": [
        0.28271739,
        21.2,
        0.28728261,
        19.76,
        0.28793478,
        18.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.65
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.82
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.71
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.97
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.58
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.24
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 54.08
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.63
              },
              {
                "metric": "lcb_code_execution",
                "score": 4.03
              },
              {
                "metric": "lcb_test_output",
                "score": 1.73
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 54.01
              },
              {
                "metric": "hard instruct",
                "score": 39.48
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.29
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 19.81
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 13,
      "name": "allenai/tulu-3-sft-personas-math",
      "domain": "math",
      "general_avg": 26.03,
      "math_avg": 27.55,
      "code_avg": 24.56,
      "reasoning_avg": 10.68,
      "overall_avg": 22.2,
      "general_scores": [
        46.06,
        31.32,
        22.68,
        0.50214286,
        55.29,
        31.6875,
        21.54,
        0.43785714,
        45.71,
        33.075,
        23.73,
        0.36571429
      ],
      "math_scores": [
        57.54,
        27.1,
        39.6,
        14.52,
        3.33,
        51.78,
        26.66,
        39.0,
        14.21,
        0.0,
        57.7,
        27.26,
        39.4,
        15.15,
        0.0
      ],
      "code_scores": [
        26.22,
        55.64,
        6.45,
        0.42,
        16.74,
        25.42,
        34.29,
        33.54,
        56.81,
        7.53,
        0.42,
        16.74,
        24.07,
        31.44,
        34.15,
        54.86,
        8.24,
        1.25,
        19.68,
        28.81,
        32.99
      ],
      "reasoning_scores": [
        0.27228261,
        21.36,
        0.25282609,
        21.92,
        0.26967391,
        20.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.02
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.03
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 22.65
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.44
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.67
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 39.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.63
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 31.3
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.77
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.41
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.7
              },
              {
                "metric": "lcb_test_output",
                "score": 17.72
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 26.1
              },
              {
                "metric": "hard instruct",
                "score": 32.91
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.26
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.09
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 14,
      "name": "dyyyyyyyy/ScaleQuest-Math",
      "domain": "math",
      "general_avg": 34.8,
      "math_avg": 48.35,
      "code_avg": 18.04,
      "reasoning_avg": 0.2,
      "overall_avg": 25.35,
      "general_scores": [
        81.28,
        23.6725,
        32.08,
        0.0,
        83.17,
        24.2225,
        30.46,
        2.06785714,
        81.97,
        23.88,
        0.03357143
      ],
      "math_scores": [
        86.66,
        63.18,
        64.2,
        21.0,
        6.67,
        89.08,
        63.24,
        64.8,
        20.57,
        0.0,
        87.41,
        63.06,
        67.8,
        20.89,
        6.67
      ],
      "code_scores": [
        16.46,
        30.74,
        2.51,
        0.0,
        7.47,
        45.76,
        20.33,
        21.34,
        35.02,
        2.15,
        0.0,
        1.81,
        47.8,
        22.91,
        22.56,
        31.91,
        0.36,
        2.3,
        2.49,
        44.41,
        20.61
      ],
      "reasoning_scores": [
        0.18402174,
        0.16,
        0.18641304,
        0.4,
        0.18130435,
        0.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.92
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 31.27
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.7
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.72
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 63.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.82
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.45
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.12
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 32.56
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.67
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.77
              },
              {
                "metric": "lcb_test_output",
                "score": 3.92
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 45.99
              },
              {
                "metric": "hard instruct",
                "score": 21.28
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.18
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 0.21
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 15,
      "name": "CodeFeedback-Filtered-Instruction",
      "domain": "code",
      "general_avg": 37.4,
      "math_avg": 13.97,
      "code_avg": 41.69,
      "reasoning_avg": 12.87,
      "overall_avg": 26.48,
      "general_scores": [
        53.9,
        45.0025,
        35.89,
        12.9128571,
        54.34,
        45.9125,
        33.82,
        17.3835714,
        54.99,
        44.7925,
        34.61,
        15.3
      ],
      "math_scores": [
        33.06,
        10.14,
        8.6,
        11.4,
        0.0,
        37.38,
        11.94,
        13.8,
        10.23,
        3.33,
        37.0,
        10.44,
        11.0,
        11.16,
        0.0
      ],
      "code_scores": [
        66.46,
        57.59,
        6.45,
        19.42,
        20.14,
        73.22,
        54.55,
        67.68,
        53.31,
        7.53,
        17.12,
        20.81,
        70.85,
        53.7,
        67.07,
        57.59,
        5.38,
        15.66,
        19.68,
        70.51,
        50.86
      ],
      "reasoning_scores": [
        0.26608696,
        25.84,
        0.24445652,
        26.96,
        0.25206522,
        23.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.41
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.24
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.77
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.2
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 35.81
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.93
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 67.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.16
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 6.45
              },
              {
                "metric": "lcb_code_execution",
                "score": 17.4
              },
              {
                "metric": "lcb_test_output",
                "score": 20.21
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 71.53
              },
              {
                "metric": "hard instruct",
                "score": 53.04
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.49
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 16,
      "name": "evol-codealpaca-v1",
      "domain": "code",
      "general_avg": 34.16,
      "math_avg": 12.4,
      "code_avg": 43.84,
      "reasoning_avg": 13.2,
      "overall_avg": 25.9,
      "general_scores": [
        47.04,
        39.87,
        34.75,
        12.6614286,
        49.14,
        41.98,
        35.69,
        12.2407143,
        47.41,
        42.24,
        35.69,
        11.1778571
      ],
      "math_scores": [
        22.06,
        10.0,
        9.2,
        16.06,
        0.0,
        24.41,
        9.92,
        8.2,
        15.29,
        6.67,
        29.04,
        9.88,
        10.4,
        14.84,
        0.0
      ],
      "code_scores": [
        69.51,
        58.37,
        5.02,
        31.11,
        18.78,
        64.07,
        53.03,
        69.51,
        59.14,
        5.38,
        36.95,
        16.74,
        68.47,
        53.36,
        69.51,
        59.14,
        6.81,
        28.6,
        23.3,
        69.15,
        54.76
      ],
      "reasoning_scores": [
        0.25380435,
        26.0,
        0.27521739,
        26.08,
        0.27380435,
        26.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.86
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.36
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.03
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.17
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.93
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.51
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 58.88
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 5.74
              },
              {
                "metric": "lcb_code_execution",
                "score": 32.22
              },
              {
                "metric": "lcb_test_output",
                "score": 19.61
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 67.23
              },
              {
                "metric": "hard instruct",
                "score": 53.72
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.27
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 26.13
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 17,
      "name": "Magicoder-Evol-Instruct-110K",
      "domain": "code",
      "general_avg": 33.9,
      "math_avg": 11.25,
      "code_avg": 42.5,
      "reasoning_avg": 13.4,
      "overall_avg": 25.26,
      "general_scores": [
        50.66,
        41.49,
        34.39,
        13.7185714,
        38.57,
        44.045,
        34.82,
        13.0278571,
        47.54,
        42.6375,
        36.22,
        9.72642857
      ],
      "math_scores": [
        22.67,
        9.46,
        7.2,
        14.7,
        0.0,
        28.35,
        9.62,
        8.2,
        15.99,
        0.0,
        22.67,
        8.22,
        7.8,
        13.91,
        0.0
      ],
      "code_scores": [
        66.46,
        59.92,
        3.58,
        27.14,
        19.68,
        68.47,
        53.95,
        70.73,
        58.37,
        4.3,
        26.1,
        24.21,
        62.71,
        53.95,
        72.56,
        56.81,
        4.3,
        23.17,
        20.81,
        60.34,
        54.85
      ],
      "reasoning_scores": [
        0.27586957,
        26.4,
        0.28597826,
        26.56,
        0.29130435,
        26.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.59
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.72
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.14
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 24.56
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.1
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.87
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.92
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 58.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.06
              },
              {
                "metric": "lcb_code_execution",
                "score": 25.47
              },
              {
                "metric": "lcb_test_output",
                "score": 21.57
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 63.84
              },
              {
                "metric": "hard instruct",
                "score": 54.25
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 26.51
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 18,
      "name": "Magicoder-OSS-Instruct-75K",
      "domain": "code",
      "general_avg": 33.31,
      "math_avg": 7.77,
      "code_avg": 28.58,
      "reasoning_avg": 8.8,
      "overall_avg": 19.61,
      "general_scores": [
        63.06,
        35.24,
        33.08,
        3.01714286,
        60.64,
        35.8975,
        33.1,
        2.31428571,
        61.74,
        34.9925,
        32.61,
        4.02285714
      ],
      "math_scores": [
        16.91,
        7.08,
        6.0,
        8.15,
        0.0,
        14.18,
        7.44,
        8.0,
        8.58,
        0.0,
        14.56,
        7.94,
        9.2,
        8.45,
        0.0
      ],
      "code_scores": [
        47.56,
        52.53,
        2.51,
        0.21,
        0.23,
        64.41,
        27.74,
        45.73,
        52.53,
        3.94,
        0.0,
        0.0,
        65.08,
        28.16,
        48.17,
        53.7,
        2.87,
        0.84,
        0.0,
        66.78,
        37.2
      ],
      "reasoning_scores": [
        0.22836957,
        18.24,
        0.2373913,
        17.6,
        0.23119565,
        16.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.81
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 35.38
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.93
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.12
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.22
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.49
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.39
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 47.15
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 52.92
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.11
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.35
              },
              {
                "metric": "lcb_test_output",
                "score": 0.08
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 65.42
              },
              {
                "metric": "hard instruct",
                "score": 31.03
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.23
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 17.36
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 19,
      "name": "Code-Feedback",
      "domain": "code",
      "general_avg": 32.25,
      "math_avg": 9.75,
      "code_avg": 33.77,
      "reasoning_avg": 8.82,
      "overall_avg": 21.15,
      "general_scores": [
        45.42,
        42.0775,
        31.85,
        11.7392857,
        54.05,
        37.0,
        33.67,
        10.0478571,
        42.18,
        37.8975,
        31.69,
        9.36214286
      ],
      "math_scores": [
        13.72,
        5.96,
        6.0,
        17.48,
        6.67,
        14.03,
        5.86,
        6.2,
        17.66,
        0.0,
        17.82,
        7.16,
        7.0,
        17.34,
        3.33
      ],
      "code_scores": [
        54.27,
        59.14,
        8.96,
        0.42,
        20.36,
        55.59,
        29.31,
        57.93,
        58.37,
        6.81,
        0.21,
        24.89,
        61.69,
        32.06,
        61.59,
        56.42,
        7.53,
        0.21,
        20.81,
        60.34,
        32.29
      ],
      "reasoning_scores": [
        0.22,
        18.08,
        0.23684783,
        15.68,
        0.22423913,
        18.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.22
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.19
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 17.49
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 57.93
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 57.98
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.77
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.28
              },
              {
                "metric": "lcb_test_output",
                "score": 22.02
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 59.21
              },
              {
                "metric": "hard instruct",
                "score": 31.22
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.23
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 17.41
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 20,
      "name": "Open-Critic-GPT",
      "domain": "code",
      "general_avg": 18.3,
      "math_avg": 2.27,
      "code_avg": 21.2,
      "reasoning_avg": 8.22,
      "overall_avg": 12.5,
      "general_scores": [
        17.96,
        24.61,
        26.21,
        3.15428571,
        15.06,
        23.915,
        25.4,
        6.20571429,
        21.4,
        23.7825,
        24.58,
        7.35571429
      ],
      "math_scores": [
        1.82,
        2.14,
        3.4,
        2.76,
        0.0,
        2.81,
        2.5,
        2.6,
        3.21,
        0.0,
        2.96,
        3.12,
        4.2,
        2.51,
        0.0
      ],
      "code_scores": [
        14.02,
        47.47,
        0.36,
        12.94,
        0.45,
        40.68,
        48.76,
        8.54,
        46.3,
        0.72,
        10.44,
        4.3,
        34.24,
        37.61,
        6.71,
        46.69,
        1.08,
        4.18,
        1.13,
        37.97,
        40.61
      ],
      "reasoning_scores": [
        0.15815217,
        16.64,
        0.14782609,
        15.04,
        0.14130435,
        17.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 24.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.57
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.53
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.59
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.4
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.83
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 9.76
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 46.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.72
              },
              {
                "metric": "lcb_code_execution",
                "score": 9.19
              },
              {
                "metric": "lcb_test_output",
                "score": 1.96
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 37.63
              },
              {
                "metric": "hard instruct",
                "score": 42.33
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 16.29
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 21,
      "name": "self-oss-instruct-sc2-exec-filter-50k",
      "domain": "code",
      "general_avg": 21.03,
      "math_avg": 7.06,
      "code_avg": 26.54,
      "reasoning_avg": 3.88,
      "overall_avg": 14.63,
      "general_scores": [
        32.95,
        30.26,
        20.24,
        4.34785714,
        24.39,
        29.7375,
        21.23,
        5.07357143
      ],
      "math_scores": [
        8.64,
        4.24,
        4.0,
        15.65,
        0.0,
        11.6,
        4.98,
        5.8,
        15.74,
        0.0
      ],
      "code_scores": [
        48.17,
        55.64,
        3.58,
        0.21,
        18.78,
        30.51,
        26.58,
        45.12,
        57.2,
        3.23,
        0.0,
        21.95,
        31.19,
        29.33
      ],
      "reasoning_scores": [
        0.14956522,
        7.6,
        0.15793478,
        7.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 30.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.74
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 4.71
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.12
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.61
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.9
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.7
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 46.64
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 56.42
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 3.4
              },
              {
                "metric": "lcb_code_execution",
                "score": 0.1
              },
              {
                "metric": "lcb_test_output",
                "score": 20.37
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 30.85
              },
              {
                "metric": "hard instruct",
                "score": 27.96
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.15
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 7.6
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 22,
      "name": "opc-sft-stage1",
      "domain": "code",
      "general_avg": 35.27,
      "math_avg": 15.37,
      "code_avg": 40.55,
      "reasoning_avg": 11.76,
      "overall_avg": 25.74,
      "general_scores": [
        41.75,
        46.4425,
        31.8,
        13.5342857,
        49.1,
        47.25,
        32.37,
        19.9864286,
        50.57,
        46.66,
        32.78,
        11.0335714
      ],
      "math_scores": [
        50.19,
        12.72,
        8.6,
        7.0,
        0.0,
        50.04,
        12.3,
        8.0,
        7.45,
        0.0,
        50.8,
        12.6,
        3.2,
        7.59,
        0.0
      ],
      "code_scores": [
        62.2,
        32.3,
        3.23,
        29.23,
        19.23,
        66.44,
        51.65,
        65.24,
        59.92,
        2.87,
        30.48,
        24.66,
        63.39,
        52.43,
        64.02,
        54.47,
        2.51,
        31.73,
        16.97,
        67.8,
        50.81
      ],
      "reasoning_scores": [
        0.39184783,
        21.84,
        0.40597826,
        23.52,
        0.35032609,
        24.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 14.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.34
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 12.54
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.35
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.82
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.9
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.87
              },
              {
                "metric": "lcb_code_execution",
                "score": 30.48
              },
              {
                "metric": "lcb_test_output",
                "score": 20.29
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 65.88
              },
              {
                "metric": "hard instruct",
                "score": 51.63
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.15
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 23,
      "name": "allenai/tulu-3-sft-personas-code",
      "domain": "code",
      "general_avg": 21.49,
      "math_avg": 4.36,
      "code_avg": 23.01,
      "reasoning_avg": 6.64,
      "overall_avg": 13.88,
      "general_scores": [
        9.02,
        37.9425,
        17.48,
        8.68857143,
        16.57,
        35.685,
        19.77,
        13.1114286,
        29.23,
        39.87,
        17.6,
        12.875
      ],
      "math_scores": [
        5.46,
        1.94,
        2.0,
        14.54,
        0.0,
        1.52,
        1.72,
        1.6,
        16.62,
        0.0,
        2.65,
        1.88,
        3.2,
        12.33,
        0.0
      ],
      "code_scores": [
        41.46,
        50.97,
        1.43,
        1.25,
        0.23,
        16.61,
        46.75,
        41.46,
        47.47,
        1.43,
        4.59,
        0.9,
        22.71,
        44.4,
        42.68,
        47.86,
        0.36,
        0.63,
        0.45,
        22.37,
        47.29
      ],
      "reasoning_scores": [
        0.10391304,
        12.08,
        0.10847826,
        10.4,
        0.09326087,
        17.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.27
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 18.28
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.56
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.21
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.85
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 2.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.5
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 41.87
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 48.77
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.07
              },
              {
                "metric": "lcb_code_execution",
                "score": 2.16
              },
              {
                "metric": "lcb_test_output",
                "score": 0.53
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 20.56
              },
              {
                "metric": "hard instruct",
                "score": 46.15
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.1
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 13.17
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 24,
      "name": "codeparrot/self-instruct-starcoder",
      "domain": "code",
      "general_avg": 27.09,
      "math_avg": 4.74,
      "code_avg": 23.88,
      "reasoning_avg": 9.39,
      "overall_avg": 16.28,
      "general_scores": [
        45.84,
        37.3625,
        18.36,
        7.40285714,
        41.66,
        39.5775,
        15.6,
        5.87214286,
        51.92,
        38.46,
        19.07,
        3.98
      ],
      "math_scores": [
        5.08,
        3.94,
        5.2,
        13.53,
        0.0,
        3.64,
        4.1,
        4.8,
        8.42,
        0.0,
        4.85,
        3.4,
        4.0,
        10.21,
        0.0
      ],
      "code_scores": [
        21.95,
        37.74,
        0.0,
        11.06,
        1.13,
        37.29,
        50.86,
        20.12,
        39.69,
        0.0,
        19.21,
        0.9,
        43.73,
        49.59,
        20.73,
        36.96,
        0.0,
        13.78,
        0.9,
        44.07,
        51.84
      ],
      "reasoning_scores": [
        0.23043478,
        17.76,
        0.26793478,
        18.96,
        0.23804348,
        18.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.47
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.47
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 17.68
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.75
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.52
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 3.81
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 20.93
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 38.13
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 14.68
              },
              {
                "metric": "lcb_test_output",
                "score": 0.98
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 41.7
              },
              {
                "metric": "hard instruct",
                "score": 50.76
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.25
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 18.53
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 25,
      "name": "nickrosh/Evol-Instruct-Code-80k-v1",
      "domain": "code",
      "general_avg": 36.01,
      "math_avg": 11.62,
      "code_avg": 40.02,
      "reasoning_avg": 10.85,
      "overall_avg": 24.62,
      "general_scores": [
        43.11,
        48.8325,
        36.7,
        9.22214286,
        47.59,
        47.2175,
        36.09,
        17.1,
        50.64,
        47.5275,
        36.17,
        11.9664286
      ],
      "math_scores": [
        35.33,
        10.36,
        10.0,
        7.0,
        0.0,
        27.6,
        10.16,
        8.6,
        6.71,
        0.0,
        31.39,
        9.84,
        9.8,
        7.45,
        0.0
      ],
      "code_scores": [
        52.44,
        53.7,
        1.08,
        23.8,
        19.68,
        77.97,
        54.67,
        46.95,
        53.7,
        2.51,
        29.02,
        12.9,
        77.97,
        53.19,
        48.78,
        53.31,
        1.08,
        26.72,
        17.65,
        78.31,
        55.03
      ],
      "reasoning_scores": [
        0.35402174,
        21.92,
        0.36195652,
        20.48,
        0.35543478,
        21.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.11
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.86
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 36.32
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.76
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.44
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 10.12
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.05
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.57
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 1.56
              },
              {
                "metric": "lcb_code_execution",
                "score": 26.51
              },
              {
                "metric": "lcb_test_output",
                "score": 16.74
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 78.08
              },
              {
                "metric": "hard instruct",
                "score": 54.3
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.36
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.33
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 26,
      "name": "TokenBender/code_instructions_122k_alpaca_style",
      "domain": "code",
      "general_avg": 35.01,
      "math_avg": 5.56,
      "code_avg": 29.98,
      "reasoning_avg": 10.45,
      "overall_avg": 20.25,
      "general_scores": [
        48.8,
        42.0,
        29.99,
        21.575,
        48.26,
        44.71,
        27.82,
        18.9085714,
        48.3,
        43.045,
        29.01,
        17.6428571
      ],
      "math_scores": [
        8.72,
        6.84,
        7.2,
        6.59,
        0.0,
        9.86,
        6.38,
        7.6,
        6.66,
        0.0,
        5.53,
        5.96,
        5.6,
        6.46,
        0.0
      ],
      "code_scores": [
        43.29,
        50.58,
        0.0,
        15.24,
        6.33,
        45.42,
        45.04,
        40.85,
        54.09,
        0.0,
        13.78,
        4.07,
        46.1,
        46.26,
        42.68,
        45.53,
        0.0,
        5.64,
        15.61,
        62.37,
        46.6
      ],
      "reasoning_scores": [
        0.28347826,
        21.6,
        0.275,
        18.96,
        0.27206522,
        21.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.45
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.25
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 28.94
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 19.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.04
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.27
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 50.07
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 11.55
              },
              {
                "metric": "lcb_test_output",
                "score": 8.67
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 51.3
              },
              {
                "metric": "hard instruct",
                "score": 45.97
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.28
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.61
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 27,
      "name": "iamtarun/python_code_instructions_18k_alpaca",
      "domain": "code",
      "general_avg": 39.97,
      "math_avg": 7.14,
      "code_avg": 33.67,
      "reasoning_avg": 11.93,
      "overall_avg": 23.18,
      "general_scores": [
        46.95,
        46.8975,
        32.6,
        29.5671429,
        51.47,
        48.0325,
        33.12,
        27.4607143,
        53.45,
        47.0875,
        34.54,
        28.4692857
      ],
      "math_scores": [
        7.51,
        8.14,
        11.0,
        7.36,
        0.0,
        10.24,
        8.12,
        9.0,
        7.38,
        3.33,
        8.34,
        7.44,
        9.0,
        6.91,
        3.33
      ],
      "code_scores": [
        37.2,
        54.47,
        0.0,
        19.62,
        9.05,
        70.85,
        53.47,
        36.59,
        52.53,
        0.0,
        10.86,
        11.31,
        64.07,
        53.88,
        39.63,
        54.47,
        0.0,
        6.05,
        13.57,
        65.76,
        53.76
      ],
      "reasoning_scores": [
        0.30804348,
        23.6,
        0.32304348,
        23.44,
        0.30043478,
        23.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.62
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.34
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.42
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.5
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.7
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.9
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.22
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 37.81
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 12.18
              },
              {
                "metric": "lcb_test_output",
                "score": 11.31
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 66.89
              },
              {
                "metric": "hard instruct",
                "score": 53.7
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 23.55
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 28,
      "name": "ajibawa-2023/Python-Code-23k-ShareGPT",
      "domain": "code",
      "general_avg": 42.16,
      "math_avg": 9.34,
      "code_avg": 36.89,
      "reasoning_avg": 13.14,
      "overall_avg": 25.38,
      "general_scores": [
        66.16,
        45.2575,
        34.66,
        21.9871429,
        66.55,
        44.4375,
        35.39,
        23.7114286,
        66.19,
        44.0875,
        34.03,
        23.5171429
      ],
      "math_scores": [
        21.46,
        9.32,
        10.0,
        8.2,
        0.0,
        21.08,
        9.22,
        8.8,
        8.31,
        0.0,
        17.06,
        8.54,
        9.8,
        8.27,
        0.0
      ],
      "code_scores": [
        51.22,
        56.42,
        7.89,
        16.91,
        22.17,
        53.56,
        57.22,
        48.78,
        55.64,
        8.24,
        5.22,
        25.34,
        53.22,
        57.96,
        47.56,
        54.09,
        6.45,
        14.61,
        23.08,
        51.19,
        57.88
      ],
      "reasoning_scores": [
        0.29184783,
        26.0,
        0.29641304,
        26.08,
        0.3075,
        25.84
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.3
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.59
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.69
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.07
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.87
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.03
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 9.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 49.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.53
              },
              {
                "metric": "lcb_code_execution",
                "score": 12.25
              },
              {
                "metric": "lcb_test_output",
                "score": 23.53
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 52.66
              },
              {
                "metric": "hard instruct",
                "score": 57.69
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.97
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 29,
      "name": "bespokelabs/Bespoke-Stratos-17k",
      "domain": "reasoning",
      "general_avg": 21.46,
      "math_avg": 20.94,
      "code_avg": 25.12,
      "reasoning_avg": 3.51,
      "overall_avg": 17.76,
      "general_scores": [
        31.42,
        28.3025,
        25.66,
        0.12142857,
        38.8,
        27.275,
        24.39,
        0.105,
        29.52,
        27.4475,
        24.46,
        0.065
      ],
      "math_scores": [
        4.4,
        42.86,
        40.8,
        15.22,
        0.0,
        3.49,
        42.54,
        40.0,
        15.06,
        0.0,
        5.08,
        44.16,
        42.2,
        15.0,
        3.33
      ],
      "code_scores": [
        38.41,
        56.03,
        9.68,
        9.6,
        0.45,
        47.12,
        22.95,
        42.68,
        52.53,
        9.32,
        2.51,
        0.0,
        45.42,
        22.43,
        39.02,
        52.92,
        8.96,
        4.18,
        0.68,
        40.68,
        22.03
      ],
      "reasoning_scores": [
        0.17032609,
        7.12,
        0.16684783,
        6.48,
        0.16467391,
        6.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 33.25
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.68
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 24.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 4.32
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.19
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.0
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 15.09
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 1.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 40.04
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 53.83
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.32
              },
              {
                "metric": "lcb_code_execution",
                "score": 5.43
              },
              {
                "metric": "lcb_test_output",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 44.41
              },
              {
                "metric": "hard instruct",
                "score": 22.47
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.17
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 6.85
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    }
  ],
  "qwen": [
    {
      "id": 1,
      "name": "GPT4All",
      "domain": "general",
      "general_avg": 51.9,
      "math_avg": 39.14,
      "code_avg": 48.07,
      "reasoning_avg": 15.45,
      "overall_avg": 38.64,
      "general_scores": [
        64.89,
        45.345,
        56.56,
        39.2021429,
        67.31,
        46.2225,
        56.54,
        39.1364286
      ],
      "math_scores": [
        80.89,
        48.22,
        47.0,
        18.9,
        0.0,
        78.47,
        49.12,
        48.8,
        20.03,
        0.0
      ],
      "code_scores": [
        75.61,
        74.71,
        12.54,
        42.17,
        30.09,
        28.47,
        67.1,
        75.61,
        73.93,
        12.54,
        41.34,
        35.75,
        35.59,
        67.55
      ],
      "reasoning_scores": [
        0.44793478,
        29.76,
        0.46315217,
        31.12
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.1
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.55
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.17
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.68
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.67
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.9
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.46
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.32
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.54
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.76
              },
              {
                "metric": "lcb_test_output",
                "score": 32.92
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 32.03
              },
              {
                "metric": "hard instruct",
                "score": 67.32
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.46
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.44
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 2,
      "name": "Alpaca",
      "domain": "general",
      "general_avg": 55.35,
      "math_avg": 37.16,
      "code_avg": 47.55,
      "reasoning_avg": 14.93,
      "overall_avg": 38.75,
      "general_scores": [
        65.44,
        54.5275,
        57.92,
        41.5621429,
        65.7,
        54.805,
        58.13,
        43.3885714,
        65.7,
        55.52,
        58.19,
        43.3107143
      ],
      "math_scores": [
        80.59,
        42.94,
        42.6,
        13.28,
        3.33,
        80.74,
        42.5,
        43.2,
        13.62,
        6.67,
        80.29,
        43.42,
        44.0,
        13.62,
        6.67
      ],
      "code_scores": [
        71.34,
        71.98,
        10.75,
        42.38,
        37.1,
        29.49,
        68.37,
        72.56,
        72.76,
        11.11,
        42.59,
        33.26,
        29.49,
        68.65,
        73.17,
        72.37,
        9.68,
        41.13,
        39.14,
        32.88,
        68.45
      ],
      "reasoning_scores": [
        0.38934783,
        28.88,
        0.39,
        29.84,
        0.39347826,
        29.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 65.61
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.95
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.75
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.54
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.95
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 43.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.51
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.36
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.51
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.03
              },
              {
                "metric": "lcb_test_output",
                "score": 36.5
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 30.62
              },
              {
                "metric": "hard instruct",
                "score": 68.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.47
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 3,
      "name": "Alpaca_GPT4",
      "domain": "general",
      "general_avg": 56.01,
      "math_avg": 44.47,
      "code_avg": 50.16,
      "reasoning_avg": 16.69,
      "overall_avg": 41.83,
      "general_scores": [
        72.8,
        51.095,
        58.83,
        42.3871429,
        72.02,
        51.565,
        58.89,
        40.5192857
      ],
      "math_scores": [
        87.95,
        53.36,
        55.2,
        21.3,
        6.67,
        86.88,
        52.8,
        52.4,
        21.48,
        6.67
      ],
      "code_scores": [
        76.22,
        72.76,
        12.54,
        37.16,
        46.61,
        36.27,
        66.0,
        77.44,
        73.15,
        13.26,
        41.54,
        47.51,
        35.59,
        66.24
      ],
      "reasoning_scores": [
        0.45934783,
        33.6,
        0.45119565,
        32.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.41
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.33
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.86
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.45
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.42
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.08
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 53.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.39
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 76.83
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.96
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.9
              },
              {
                "metric": "lcb_code_execution",
                "score": 39.35
              },
              {
                "metric": "lcb_test_output",
                "score": 47.06
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 35.93
              },
              {
                "metric": "hard instruct",
                "score": 66.12
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.46
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.92
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 4,
      "name": "WizardLM_evol_instruct_V2_196k (实际只有143k)",
      "domain": "general",
      "general_avg": 55.43,
      "math_avg": 39.17,
      "code_avg": 49.83,
      "reasoning_avg": 18.06,
      "overall_avg": 40.62,
      "general_scores": [
        71.25,
        52.5275,
        58.1,
        40.08,
        72.25,
        53.2075,
        58.15,
        39.6814286,
        71.33,
        51.6175,
        57.95,
        39.0557143
      ],
      "math_scores": [
        82.18,
        47.22,
        47.6,
        19.26,
        0.0,
        81.2,
        48.04,
        48.8,
        19.31,
        0.0,
        80.89,
        47.14,
        46.8,
        19.13,
        0.0
      ],
      "code_scores": [
        80.49,
        74.71,
        11.11,
        40.71,
        42.99,
        32.2,
        67.14,
        78.05,
        74.32,
        10.75,
        41.96,
        42.08,
        35.25,
        66.9,
        78.66,
        72.76,
        11.83,
        40.71,
        44.57,
        32.2,
        67.14
      ],
      "reasoning_scores": [
        0.41554348,
        35.36,
        0.41228261,
        36.4,
        0.41163043,
        35.36
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.61
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.45
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.07
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.61
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.42
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.47
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.23
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.23
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.13
              },
              {
                "metric": "lcb_test_output",
                "score": 43.21
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 33.22
              },
              {
                "metric": "hard instruct",
                "score": 67.06
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.71
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 5,
      "name": "LIMA",
      "domain": "general",
      "general_avg": 52.44,
      "math_avg": 41.23,
      "code_avg": 49.99,
      "reasoning_avg": 14.3,
      "overall_avg": 39.49,
      "general_scores": [
        77.31,
        43.3625,
        57.86,
        29.9021429,
        76.91,
        41.78,
        57.18,
        33.8178571,
        77.2,
        43.195,
        57.96,
        32.7728571
      ],
      "math_scores": [
        72.33,
        50.04,
        49.6,
        22.99,
        10.0,
        71.11,
        50.36,
        48.6,
        22.9,
        16.67,
        71.19,
        50.08,
        46.0,
        23.24,
        13.33
      ],
      "code_scores": [
        74.39,
        70.82,
        11.47,
        45.93,
        42.08,
        34.24,
        67.42,
        72.56,
        69.65,
        10.75,
        46.35,
        51.13,
        34.92,
        67.46,
        73.78,
        69.65,
        12.19,
        45.93,
        49.32,
        32.88,
        66.94
      ],
      "reasoning_scores": [
        0.40869565,
        28.24,
        0.39793478,
        26.32,
        0.40413043,
        30.0
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.14
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.78
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.54
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.16
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.07
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.04
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.58
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.04
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 46.07
              },
              {
                "metric": "lcb_test_output",
                "score": 47.51
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 34.01
              },
              {
                "metric": "hard instruct",
                "score": 67.27
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.4
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 28.19
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 6,
      "name": "microsoft/orca-agentinstruct-1M-v1",
      "domain": "general",
      "general_avg": 58.36,
      "math_avg": 47.62,
      "code_avg": 56.81,
      "reasoning_avg": 21.38,
      "overall_avg": 46.04,
      "general_scores": [
        76.72,
        57.77,
        55.71,
        42.8821429,
        75.95,
        58.145,
        55.19,
        42.1885714,
        76.05,
        60.0675,
        55.23,
        44.3628571
      ],
      "math_scores": [
        86.88,
        59.16,
        59.4,
        21.27,
        10.0,
        87.57,
        59.84,
        60.2,
        21.18,
        13.33,
        85.67,
        60.6,
        61.2,
        21.36,
        6.67
      ],
      "code_scores": [
        75.0,
        71.98,
        12.54,
        39.67,
        40.95,
        87.46,
        69.34,
        75.0,
        74.32,
        12.9,
        34.86,
        40.95,
        87.12,
        70.42,
        72.56,
        74.32,
        14.34,
        40.71,
        40.05,
        89.15,
        69.39
      ],
      "reasoning_scores": [
        0.37869565,
        41.52,
        0.37706522,
        43.2,
        0.37978261,
        42.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.24
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 58.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.38
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.14
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 86.71
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 59.87
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.27
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 10.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.54
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.41
              },
              {
                "metric": "lcb_test_output",
                "score": 40.65
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 87.91
              },
              {
                "metric": "hard instruct",
                "score": 69.72
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 42.37
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 7,
      "name": "OpenHermes-2.5",
      "domain": "general",
      "general_avg": 53.96,
      "math_avg": 35.28,
      "code_avg": 47.79,
      "reasoning_avg": 15.21,
      "overall_avg": 38.06,
      "general_scores": [
        70.36,
        56.3125,
        57.35,
        34.2271429,
        70.12,
        55.5125,
        57.8,
        35.0807143,
        70.74,
        54.6775,
        58.39,
        26.9914286
      ],
      "math_scores": [
        82.71,
        41.02,
        40.2,
        19.76,
        3.33,
        82.41,
        34.3,
        29.4,
        19.67,
        0.0,
        84.08,
        35.48,
        34.2,
        19.33,
        3.33
      ],
      "code_scores": [
        77.44,
        70.82,
        12.9,
        36.95,
        43.67,
        29.49,
        67.31,
        73.17,
        71.21,
        12.19,
        33.82,
        41.18,
        33.22,
        67.85,
        71.34,
        69.65,
        12.9,
        33.4,
        40.95,
        37.63,
        66.4
      ],
      "reasoning_scores": [
        0.41141304,
        32.8,
        0.41532609,
        27.52,
        0.4398913,
        29.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.41
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 55.5
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.85
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.07
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.93
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 34.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.59
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.98
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.56
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 34.72
              },
              {
                "metric": "lcb_test_output",
                "score": 41.93
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 33.45
              },
              {
                "metric": "hard instruct",
                "score": 67.19
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.0
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 8,
      "name": "SelfInstruct",
      "domain": "general",
      "general_avg": 39.52,
      "math_avg": 6.75,
      "code_avg": 23.14,
      "reasoning_avg": 10.21,
      "overall_avg": 19.9,
      "general_scores": [
        49.32,
        39.67,
        30.6535714,
        48.58,
        42.07,
        29.4264286,
        46.43,
        43.67,
        25.8307143
      ],
      "math_scores": [
        5.16,
        7.2,
        7.8,
        6.67,
        4.78,
        6.5,
        5.8,
        6.67,
        10.46,
        7.06,
        6.2,
        6.67
      ],
      "code_scores": [
        13.41,
        28.4,
        0.0,
        31.73,
        0.0,
        32.54,
        54.05,
        14.63,
        28.4,
        0.0,
        30.69,
        0.0,
        33.56,
        54.92,
        12.8,
        27.63,
        0.0,
        30.9,
        0.0,
        37.63,
        54.57
      ],
      "reasoning_scores": [
        0.33826087,
        20.48,
        0.33630435,
        19.28,
        0.33554348,
        20.48
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.11
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 41.8
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 28.64
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.8
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.92
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 6.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 6.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 13.61
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 28.14
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 31.11
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 34.58
              },
              {
                "metric": "hard instruct",
                "score": 54.51
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.34
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 20.08
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 9,
      "name": "AutoCoT",
      "domain": "general",
      "general_avg": 35.76,
      "math_avg": 7.48,
      "code_avg": 27.92,
      "reasoning_avg": 3.83,
      "overall_avg": 18.75,
      "general_scores": [
        76.37,
        18.9625,
        48.83,
        0.00785714,
        75.92,
        17.9,
        48.41,
        0.00857143,
        76.1,
        19.6425,
        46.9,
        0.01571429
      ],
      "math_scores": [
        0.99,
        8.42,
        14.2,
        13.39,
        0.0,
        0.83,
        7.74,
        12.4,
        13.91,
        0.0,
        1.59,
        9.1,
        15.8,
        13.87,
        0.0
      ],
      "code_scores": [
        9.76,
        72.76,
        4.3,
        4.18,
        0.9,
        42.37,
        51.89,
        51.68,
        9.76,
        72.37,
        3.94,
        4.18,
        0.23,
        40.34,
        55.03,
        15.24,
        71.21,
        4.3,
        3.55,
        1.13,
        40.0,
        55.03
      ],
      "reasoning_scores": [
        0.29358696,
        4.08,
        0.29782609,
        8.56,
        0.31021739,
        8.4,
        0.31021739,
        8.4
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.13
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 18.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 48.05
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.01
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 1.14
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 8.42
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 14.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.72
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 11.59
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.11
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 4.18
              },
              {
                "metric": "lcb_code_execution",
                "score": 3.97
              },
              {
                "metric": "lcb_test_output",
                "score": 0.75
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 40.9
              },
              {
                "metric": "hard instruct",
                "score": 53.41
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.3
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 7.36
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 10,
      "name": "MathInstruct",
      "domain": "math",
      "general_avg": 55.72,
      "math_avg": 37.08,
      "code_avg": 49.3,
      "reasoning_avg": 15.21,
      "overall_avg": 39.33,
      "general_scores": [
        77.55,
        50.54,
        56.73,
        38.2635714,
        76.59,
        50.905,
        56.77,
        35.5721429,
        77.55,
        51.535,
        56.75,
        39.8385714
      ],
      "math_scores": [
        79.08,
        45.06,
        41.0,
        18.61,
        0.0,
        78.77,
        46.1,
        42.8,
        19.4,
        0.0,
        76.88,
        46.22,
        43.0,
        19.29,
        0.0
      ],
      "code_scores": [
        78.66,
        71.21,
        13.26,
        41.54,
        41.63,
        33.22,
        67.91,
        76.83,
        73.93,
        11.11,
        41.34,
        42.08,
        32.54,
        67.21,
        76.83,
        72.37,
        10.39,
        42.8,
        42.76,
        29.49,
        68.22
      ],
      "reasoning_scores": [
        0.47076087,
        28.96,
        0.44173913,
        31.28,
        0.42152174,
        29.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.23
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.99
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.75
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.89
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 78.24
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.79
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.27
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.1
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.5
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.59
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.89
              },
              {
                "metric": "lcb_test_output",
                "score": 42.16
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 31.75
              },
              {
                "metric": "hard instruct",
                "score": 67.78
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.97
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 11,
      "name": "Orca-Math",
      "domain": "math",
      "general_avg": 56.93,
      "math_avg": 39.59,
      "code_avg": 50.33,
      "reasoning_avg": 20.0,
      "overall_avg": 41.71,
      "general_scores": [
        76.89,
        53.39,
        58.6,
        46.8271429,
        76.06,
        52.1325,
        58.76,
        47.5414286,
        75.05,
        50.765,
        57.84,
        29.2514286
      ],
      "math_scores": [
        75.36,
        52.54,
        51.2,
        24.44,
        6.67,
        73.92,
        51.36,
        49.6,
        24.07,
        13.33,
        62.4,
        42.26,
        43.0,
        20.39,
        3.33
      ],
      "code_scores": [
        78.05,
        75.1,
        13.98,
        42.38,
        49.32,
        39.66,
        68.32,
        79.27,
        75.1,
        13.26,
        43.84,
        49.1,
        33.9,
        68.11,
        65.24,
        71.98,
        12.54,
        42.17,
        42.53,
        26.44,
        66.65
      ],
      "reasoning_scores": [
        0.41423913,
        39.44,
        0.40315217,
        38.08,
        0.39,
        41.28
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.0
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 52.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.4
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.21
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 70.56
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.72
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 47.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.97
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.19
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.06
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.26
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.8
              },
              {
                "metric": "lcb_test_output",
                "score": 46.98
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 33.33
              },
              {
                "metric": "hard instruct",
                "score": 67.69
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.4
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 39.6
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 12,
      "name": "camel-ai/math",
      "domain": "math",
      "general_avg": 51.69,
      "math_avg": 34.4,
      "code_avg": 48.36,
      "reasoning_avg": 16.84,
      "overall_avg": 37.82,
      "general_scores": [
        77.61,
        46.9175,
        50.87,
        32.205,
        77.95,
        46.815,
        50.53,
        32.2257143,
        76.95,
        45.7325,
        50.67,
        31.8085714
      ],
      "math_scores": [
        59.21,
        45.0,
        43.8,
        18.68,
        6.67,
        62.85,
        45.44,
        45.8,
        18.97,
        0.0,
        60.96,
        45.54,
        44.8,
        18.34,
        0.0
      ],
      "code_scores": [
        75.61,
        71.6,
        12.19,
        43.84,
        29.64,
        32.2,
        67.88,
        68.62,
        75.0,
        73.15,
        11.83,
        43.42,
        35.29,
        33.56,
        68.98,
        75.61,
        70.43,
        12.19,
        44.05,
        35.29,
        35.25
      ],
      "reasoning_scores": [
        0.4225,
        32.56,
        0.42228261,
        33.52,
        0.41619565,
        33.68
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 77.5
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.49
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 50.69
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 32.08
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.01
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.8
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.66
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 2.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 75.41
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.07
              },
              {
                "metric": "lcb_code_execution",
                "score": 43.77
              },
              {
                "metric": "lcb_test_output",
                "score": 33.41
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 33.67
              },
              {
                "metric": "hard instruct",
                "score": 68.49
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.25
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 13,
      "name": "SkunkworksAI/reasoning-0.01",
      "domain": "math",
      "general_avg": 38.26,
      "math_avg": 10.48,
      "code_avg": 31.23,
      "reasoning_avg": 13.45,
      "overall_avg": 23.36,
      "general_scores": [
        57.63,
        37.445,
        48.61,
        7.2,
        59.14,
        37.19,
        48.41,
        9.63714286,
        59.3,
        38.0825,
        49.5,
        6.98
      ],
      "math_scores": [
        14.1,
        11.3,
        6.8,
        19.29,
        0.0,
        13.12,
        11.62,
        7.8,
        19.38,
        0.0,
        14.25,
        12.12,
        7.4,
        20.01,
        0.0
      ],
      "code_scores": [
        2.44,
        72.76,
        8.6,
        6.47,
        38.69,
        29.15,
        61.45,
        4.27,
        71.6,
        6.45,
        7.52,
        31.22,
        31.19,
        60.35,
        6.71,
        71.6,
        8.24,
        4.8,
        34.39,
        36.95,
        61.07
      ],
      "reasoning_scores": [
        0.30858696,
        25.84,
        0.30521739,
        25.36,
        0.30413043,
        28.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.69
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.57
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 48.84
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.94
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 13.82
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 11.68
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 7.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.56
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 4.47
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 7.76
              },
              {
                "metric": "lcb_code_execution",
                "score": 6.26
              },
              {
                "metric": "lcb_test_output",
                "score": 34.77
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 32.43
              },
              {
                "metric": "hard instruct",
                "score": 60.96
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 26.59
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 14,
      "name": "allenai/tulu-3-sft-personas-math",
      "domain": "math",
      "general_avg": 43.67,
      "math_avg": 41.51,
      "code_avg": 44.3,
      "reasoning_avg": 13.08,
      "overall_avg": 35.64,
      "general_scores": [
        75.73,
        47.58,
        48.91,
        0.10928571,
        75.64,
        48.9925,
        48.55,
        0.09428571,
        76.02,
        49.7,
        51.68,
        1.01642857
      ],
      "math_scores": [
        71.95,
        27.44,
        67.6,
        26.45,
        6.67,
        69.75,
        29.6,
        67.8,
        27.35,
        16.67,
        74.91,
        27.58,
        69.0,
        26.51,
        13.33
      ],
      "code_scores": [
        35.37,
        70.04,
        12.9,
        40.71,
        42.99,
        31.86,
        68.7,
        25.61,
        71.98,
        12.19,
        40.92,
        44.8,
        29.83,
        68.11,
        68.29,
        73.15,
        13.98,
        25.89,
        46.15,
        36.61,
        70.3
      ],
      "reasoning_scores": [
        0.35880435,
        24.56,
        0.36271739,
        25.84,
        0.37706522,
        26.96
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.8
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 48.76
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 49.71
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.41
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.2
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 28.21
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.13
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 26.77
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 43.09
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.72
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.02
              },
              {
                "metric": "lcb_code_execution",
                "score": 35.84
              },
              {
                "metric": "lcb_test_output",
                "score": 44.65
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 32.77
              },
              {
                "metric": "hard instruct",
                "score": 69.04
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.37
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 25.79
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 15,
      "name": "dyyyyyyyy/ScaleQuest-Math",
      "domain": "math",
      "general_avg": 42.36,
      "math_avg": 54.56,
      "code_avg": 39.89,
      "reasoning_avg": 0.68,
      "overall_avg": 34.37,
      "general_scores": [
        87.67,
        35.41,
        46.59,
        0.00785714,
        87.29,
        36.31,
        45.59,
        0.0,
        87.32,
        37.18,
        44.91,
        0.0
      ],
      "math_scores": [
        88.1,
        72.64,
        74.0,
        26.78,
        13.33,
        88.17,
        72.22,
        74.0,
        27.12,
        10.0,
        88.1,
        72.2,
        73.8,
        27.87,
        10.0
      ],
      "code_scores": [
        64.63,
        73.54,
        9.68,
        20.88,
        13.57,
        33.9,
        63.97,
        61.59,
        72.76,
        11.47,
        20.46,
        11.76,
        37.29,
        64.9,
        64.02,
        73.54,
        9.32,
        18.79,
        14.03,
        32.88,
        64.72
      ],
      "reasoning_scores": [
        0.22065217,
        1.44,
        0.2201087,
        0.96,
        0.21913043,
        1.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 87.43
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 36.3
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 45.7
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0.0
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 88.12
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.35
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 63.41
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.28
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.16
              },
              {
                "metric": "lcb_code_execution",
                "score": 20.04
              },
              {
                "metric": "lcb_test_output",
                "score": 13.12
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 34.69
              },
              {
                "metric": "hard instruct",
                "score": 64.53
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.22
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 1.15
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 16,
      "name": "CodeAlpaca",
      "domain": "code",
      "general_avg": 55.97,
      "math_avg": 42.77,
      "code_avg": 49.1,
      "reasoning_avg": 17.43,
      "overall_avg": 41.32,
      "general_scores": [
        73.46,
        50.4625,
        58.39,
        40.425,
        72.79,
        51.2425,
        58.7,
        42.015,
        73.71,
        52.2175,
        58.47,
        39.75
      ],
      "math_scores": [
        85.97,
        48.64,
        51.8,
        21.03,
        6.67,
        85.75,
        50.6,
        52.6,
        20.05,
        3.33,
        85.22,
        49.72,
        53.0,
        20.53,
        6.67
      ],
      "code_scores": [
        80.49,
        72.37,
        11.83,
        37.58,
        42.76,
        32.2,
        67.97,
        78.66,
        71.6,
        12.19,
        36.95,
        43.21,
        30.17,
        68.39,
        78.05,
        73.15,
        11.47,
        38.41,
        43.44,
        31.86,
        68.34
      ],
      "reasoning_scores": [
        0.43880435,
        33.6,
        0.43891304,
        34.64,
        0.4401087,
        35.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.32
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.52
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.73
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.54
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.83
              },
              {
                "metric": "lcb_code_execution",
                "score": 37.65
              },
              {
                "metric": "lcb_test_output",
                "score": 43.14
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 31.41
              },
              {
                "metric": "hard instruct",
                "score": 68.23
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.43
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 17,
      "name": "opc-sft-stage2",
      "domain": "code",
      "general_avg": 57.83,
      "math_avg": 46.32,
      "code_avg": 47.17,
      "reasoning_avg": 17.79,
      "overall_avg": 42.28,
      "general_scores": [
        73.95,
        54.0475,
        57.61,
        45.75,
        73.28,
        55.2325,
        58.17,
        45.3114286,
        72.35,
        55.42,
        58.46,
        44.4035714
      ],
      "math_scores": [
        86.81,
        49.72,
        51.2,
        21.12,
        26.67,
        86.28,
        49.78,
        51.6,
        21.43,
        20.0,
        84.69,
        48.5,
        52.2,
        21.43,
        23.33
      ],
      "code_scores": [
        78.66,
        75.88,
        13.26,
        40.92,
        16.52,
        33.56,
        67.17,
        79.27,
        72.76,
        13.98,
        41.13,
        17.65,
        33.22,
        67.49,
        82.32,
        75.49,
        14.34,
        38.0,
        28.28,
        32.54,
        68.17
      ],
      "reasoning_scores": [
        0.44717391,
        35.6,
        0.43336957,
        35.76,
        0.43282609,
        34.08
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.19
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 54.9
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.16
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.93
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.33
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 23.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 80.08
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 74.71
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.86
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.02
              },
              {
                "metric": "lcb_test_output",
                "score": 20.82
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 33.11
              },
              {
                "metric": "hard instruct",
                "score": 67.61
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.15
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 18,
      "name": "CodeFeedback-Filtered-Instruction",
      "domain": "code",
      "general_avg": 55.87,
      "math_avg": 42.32,
      "code_avg": 49.12,
      "reasoning_avg": 16.94,
      "overall_avg": 41.06,
      "general_scores": [
        71.55,
        51.1425,
        57.7,
        42.3457143,
        72.22,
        50.4425,
        58.0,
        43.0657143,
        72.94,
        50.2575,
        58.04,
        42.7221429
      ],
      "math_scores": [
        81.96,
        49.72,
        52.6,
        20.57,
        10.0,
        79.76,
        49.92,
        50.8,
        20.37,
        6.67,
        82.11,
        48.64,
        51.4,
        20.23,
        10.0
      ],
      "code_scores": [
        78.66,
        74.32,
        13.62,
        39.04,
        43.67,
        28.47,
        67.62,
        79.27,
        70.04,
        14.7,
        38.41,
        44.34,
        28.14,
        67.7,
        80.49,
        70.82,
        13.98,
        38.2,
        41.86,
        29.83,
        68.4
      ],
      "reasoning_scores": [
        0.41380435,
        33.44,
        0.41195652,
        32.8,
        0.42804348,
        34.16
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.24
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.61
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.91
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.71
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.28
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.43
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 51.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.39
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.89
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.47
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.73
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.1
              },
              {
                "metric": "lcb_code_execution",
                "score": 38.55
              },
              {
                "metric": "lcb_test_output",
                "score": 43.29
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 28.81
              },
              {
                "metric": "hard instruct",
                "score": 67.91
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.47
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 19,
      "name": "evol-codealpaca-v1",
      "domain": "code",
      "general_avg": 55.97,
      "math_avg": 42.77,
      "code_avg": 49.1,
      "reasoning_avg": 17.43,
      "overall_avg": 41.32,
      "general_scores": [
        73.46,
        50.4625,
        58.39,
        40.425,
        72.79,
        51.2425,
        58.7,
        42.015,
        73.71,
        52.2175,
        58.47,
        39.75
      ],
      "math_scores": [
        85.97,
        48.64,
        51.8,
        21.03,
        6.67,
        85.75,
        50.6,
        52.6,
        20.05,
        3.33,
        85.22,
        49.72,
        53.0,
        20.53,
        6.67
      ],
      "code_scores": [
        80.49,
        72.37,
        11.83,
        37.58,
        42.76,
        32.2,
        67.97,
        78.66,
        71.6,
        12.19,
        36.95,
        43.21,
        30.17,
        68.39,
        78.05,
        73.15,
        11.47,
        38.41,
        43.44,
        31.86,
        68.34
      ],
      "reasoning_scores": [
        0.43880435,
        33.6,
        0.43891304,
        34.64,
        0.4401087,
        35.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.32
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.31
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.52
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 40.73
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 49.65
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.47
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.54
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.07
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.83
              },
              {
                "metric": "lcb_code_execution",
                "score": 37.65
              },
              {
                "metric": "lcb_test_output",
                "score": 43.14
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 31.41
              },
              {
                "metric": "hard instruct",
                "score": 68.23
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 34.43
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 20,
      "name": "Magicoder-Evol-Instruct-110K",
      "domain": "code",
      "general_avg": 56.18,
      "math_avg": 43.35,
      "code_avg": 48.81,
      "reasoning_avg": 17.82,
      "overall_avg": 41.54,
      "general_scores": [
        72.15,
        52.31,
        59.02,
        40.0664286,
        73.45,
        50.7375,
        58.68,
        41.9592857,
        73.67,
        52.3975,
        58.3,
        41.36
      ],
      "math_scores": [
        84.69,
        50.9,
        53.4,
        20.71,
        10.0,
        85.14,
        49.32,
        50.6,
        19.87,
        6.67,
        85.22,
        49.98,
        52.6,
        21.14,
        10.0
      ],
      "code_scores": [
        78.05,
        73.54,
        11.47,
        38.41,
        45.48,
        33.56,
        67.98,
        79.88,
        71.21,
        12.19,
        35.49,
        43.21,
        27.8,
        67.78,
        78.66,
        72.37,
        10.75,
        36.53,
        42.76,
        30.17,
        67.71
      ],
      "reasoning_scores": [
        0.44086957,
        34.24,
        0.44271739,
        34.48,
        0.42641304,
        36.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 73.09
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.82
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.67
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 41.13
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 85.02
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 50.07
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 52.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 20.57
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 8.89
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.37
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 11.47
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.81
              },
              {
                "metric": "lcb_test_output",
                "score": 43.82
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 30.51
              },
              {
                "metric": "hard instruct",
                "score": 67.82
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 35.2
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 21,
      "name": "Magicoder-OSS-Instruct-75K",
      "domain": "code",
      "general_avg": 57.03,
      "math_avg": 48.88,
      "code_avg": 49.39,
      "reasoning_avg": 14.67,
      "overall_avg": 42.49,
      "general_scores": [
        74.18,
        50.5175,
        58.27,
        44.5807143,
        75.17,
        49.6875,
        58.73,
        45.1207143
      ],
      "math_scores": [
        77.86,
        61.66,
        61.2,
        26.31,
        16.67,
        81.27,
        61.66,
        60.0,
        25.5,
        16.67
      ],
      "code_scores": [
        79.27,
        72.37,
        11.83,
        42.8,
        35.29,
        35.25,
        69.52,
        79.88,
        71.6,
        12.54,
        42.38,
        33.94,
        34.24,
        70.48
      ],
      "reasoning_scores": [
        0.41336957,
        29.12,
        0.40923913,
        28.72
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.68
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.1
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.5
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 44.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 79.56
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.66
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 60.6
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.9
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 16.67
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 79.57
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.98
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.18
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.59
              },
              {
                "metric": "lcb_test_output",
                "score": 34.61
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 34.75
              },
              {
                "metric": "hard instruct",
                "score": 70.0
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 28.92
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 22,
      "name": "Code-Feedback",
      "domain": "code",
      "general_avg": 51.47,
      "math_avg": 34.24,
      "code_avg": 46.82,
      "reasoning_avg": 14.79,
      "overall_avg": 36.83,
      "general_scores": [
        75.82,
        48.3275,
        55.52,
        27.7192857,
        76.13,
        48.3925,
        54.78,
        25.4685714,
        76.39,
        45.895,
        54.65,
        28.5435714
      ],
      "math_scores": [
        76.8,
        30.94,
        29.0,
        22.34,
        10.0,
        75.44,
        31.74,
        30.2,
        23.06,
        13.33,
        73.92,
        31.56,
        30.4,
        21.59,
        13.33
      ],
      "code_scores": [
        75.0,
        73.15,
        12.19,
        24.01,
        44.34,
        33.22,
        68.02,
        73.17,
        73.54,
        12.9,
        26.1,
        41.4,
        30.17,
        67.36,
        71.34,
        71.98,
        13.26,
        27.35,
        41.18,
        35.93,
        67.51
      ],
      "reasoning_scores": [
        0.32086957,
        28.96,
        0.32782609,
        30.16,
        0.32369565,
        28.64
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.11
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 47.54
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 54.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 27.24
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.39
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 31.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 29.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.33
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 12.22
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.17
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 72.89
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.78
              },
              {
                "metric": "lcb_code_execution",
                "score": 25.82
              },
              {
                "metric": "lcb_test_output",
                "score": 42.31
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 33.11
              },
              {
                "metric": "hard instruct",
                "score": 67.63
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.32
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.25
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 23,
      "name": "Open-Critic-GPT",
      "domain": "code",
      "general_avg": 49.96,
      "math_avg": 36.48,
      "code_avg": 42.52,
      "reasoning_avg": 10.77,
      "overall_avg": 34.93,
      "general_scores": [
        71.86,
        39.445,
        44.32,
        42.75,
        72.08,
        39.645,
        45.15,
        44.2685714,
        72.69,
        39.16,
        44.39,
        43.7485714
      ],
      "math_scores": [
        68.08,
        40.54,
        36.6,
        25.09,
        6.67,
        69.9,
        42.76,
        39.6,
        25.43,
        10.0,
        66.26,
        39.7,
        34.6,
        25.25,
        16.67
      ],
      "code_scores": [
        69.51,
        68.87,
        9.32,
        40.71,
        0.45,
        33.22,
        68.44,
        70.73,
        66.93,
        11.83,
        46.76,
        3.17,
        34.24,
        68.19,
        69.51,
        66.54,
        8.24,
        48.43,
        8.37,
        31.53,
        67.94
      ],
      "reasoning_scores": [
        0.38967391,
        19.6,
        0.38804348,
        21.92,
        0.39413043,
        21.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 72.21
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 39.42
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 44.62
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 43.59
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 41.0
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 36.93
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.26
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 11.11
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 69.92
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 9.8
              },
              {
                "metric": "lcb_code_execution",
                "score": 45.3
              },
              {
                "metric": "lcb_test_output",
                "score": 4.0
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 33.0
              },
              {
                "metric": "hard instruct",
                "score": 68.19
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.39
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 21.15
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 24,
      "name": "self-oss-instruct-sc2-exec-filter-50k",
      "domain": "code",
      "general_avg": 53.39,
      "math_avg": 35.08,
      "code_avg": 44.68,
      "reasoning_avg": 9.94,
      "overall_avg": 35.77,
      "general_scores": [
        75.76,
        45.205,
        55.36,
        37.7707143,
        75.5,
        46.14,
        54.79,
        37.9335714,
        75.76,
        45.295,
        55.3,
        35.8214286
      ],
      "math_scores": [
        61.11,
        45.02,
        42.8,
        23.28,
        6.67,
        55.5,
        43.84,
        42.4,
        23.01,
        6.67,
        56.33,
        44.3,
        41.4,
        23.92,
        10.0
      ],
      "code_scores": [
        72.56,
        73.54,
        13.98,
        24.43,
        28.73,
        31.19,
        68.64,
        71.95,
        71.21,
        13.98,
        25.47,
        26.47,
        31.86,
        67.61,
        71.95,
        71.21,
        14.34,
        27.14,
        29.41,
        34.24,
        68.38
      ],
      "reasoning_scores": [
        0.38173913,
        19.6,
        0.38108696,
        18.56,
        0.37456522,
        20.32
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.67
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 45.55
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.15
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.18
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 57.65
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 44.39
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.4
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 72.15
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 71.99
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 14.1
              },
              {
                "metric": "lcb_code_execution",
                "score": 25.68
              },
              {
                "metric": "lcb_test_output",
                "score": 28.2
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 32.43
              },
              {
                "metric": "hard instruct",
                "score": 68.21
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 19.49
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 25,
      "name": "opc-sft-stage1",
      "domain": "code",
      "general_avg": 56.5,
      "math_avg": 34.35,
      "code_avg": 46.86,
      "reasoning_avg": 16.98,
      "overall_avg": 38.67,
      "general_scores": [
        74.85,
        51.775,
        58.8,
        42.7107143,
        74.91,
        50.195,
        58.67,
        41.5028571,
        73.57,
        51.02,
        57.95,
        42.0721429
      ],
      "math_scores": [
        58.68,
        46.02,
        41.6,
        20.53,
        3.33,
        54.06,
        44.66,
        42.4,
        21.5,
        3.33,
        62.77,
        47.18,
        44.2,
        21.61,
        3.33
      ],
      "code_scores": [
        76.83,
        71.6,
        13.26,
        37.16,
        43.21,
        25.76,
        65.39,
        78.05,
        71.6,
        12.19,
        27.97,
        42.76,
        32.88,
        65.57,
        81.71,
        69.26,
        11.47,
        18.58,
        44.12,
        28.14,
        66.51
      ],
      "reasoning_scores": [
        0.43847826,
        33.52,
        0.43271739,
        34.16,
        0.4475,
        32.88
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.44
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.0
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 58.47
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.1
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 58.5
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 45.95
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 42.73
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 21.21
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 3.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 78.86
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.82
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.31
              },
              {
                "metric": "lcb_code_execution",
                "score": 27.9
              },
              {
                "metric": "lcb_test_output",
                "score": 43.36
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 28.93
              },
              {
                "metric": "hard instruct",
                "score": 65.82
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.44
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.52
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 26,
      "name": "allenai/tulu-3-sft-personas-code",
      "domain": "code",
      "general_avg": 54.71,
      "math_avg": 51.5,
      "code_avg": 43.12,
      "reasoning_avg": 15.26,
      "overall_avg": 41.15,
      "general_scores": [
        72.01,
        51.76,
        59.64,
        37.0535714,
        70.65,
        51.2125,
        58.86,
        37.1528571,
        70.48,
        51.0075,
        58.75,
        37.935
      ],
      "math_scores": [
        83.17,
        65.76,
        66.6,
        27.48,
        13.33,
        81.58,
        66.6,
        70.6,
        27.08,
        13.33,
        81.5,
        66.88,
        67.8,
        27.51,
        13.33
      ],
      "code_scores": [
        77.44,
        73.93,
        10.04,
        43.01,
        0.0,
        28.47,
        69.63,
        75.61,
        73.54,
        11.47,
        40.92,
        0.0,
        30.85,
        69.84,
        78.66,
        73.54,
        9.68,
        38.41,
        0.0,
        30.85,
        69.71
      ],
      "reasoning_scores": [
        0.42565217,
        28.64,
        0.41673913,
        29.76,
        0.42467391,
        31.92
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 71.05
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.33
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 59.08
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 37.38
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 82.08
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 66.41
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 68.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 27.36
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.24
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 73.67
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 10.4
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.78
              },
              {
                "metric": "lcb_test_output",
                "score": 0.0
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 30.06
              },
              {
                "metric": "hard instruct",
                "score": 69.73
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.42
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 30.11
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 27,
      "name": "codeparrot/self-instruct-starcoder",
      "domain": "code",
      "general_avg": 34.86,
      "math_avg": 19.28,
      "code_avg": 32.57,
      "reasoning_avg": 11.42,
      "overall_avg": 24.53,
      "general_scores": [
        19.9,
        38.3025,
        51.93,
        26.8121429,
        26.92,
        38.275,
        51.86,
        25.6107143,
        23.18,
        39.3875,
        51.8,
        24.3114286
      ],
      "math_scores": [
        25.78,
        21.88,
        25.2,
        20.69,
        3.33,
        21.46,
        22.5,
        25.8,
        18.16,
        6.67,
        23.73,
        22.08,
        26.0,
        19.26,
        6.67
      ],
      "code_scores": [
        42.68,
        52.14,
        0.0,
        35.7,
        3.85,
        34.24,
        60.38,
        42.68,
        51.36,
        0.0,
        36.53,
        4.07,
        30.85,
        60.26,
        42.68,
        49.42,
        0.0,
        36.53,
        2.26,
        37.63,
        60.7
      ],
      "reasoning_scores": [
        0.31304348,
        22.32,
        0.3151087,
        23.04,
        0.31413043,
        22.24
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.33
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 38.66
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 51.86
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 25.58
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.66
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 22.15
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.67
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 19.37
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 5.56
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 42.68
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 50.97
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.0
              },
              {
                "metric": "lcb_code_execution",
                "score": 36.25
              },
              {
                "metric": "lcb_test_output",
                "score": 3.39
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 34.24
              },
              {
                "metric": "hard instruct",
                "score": 60.45
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.31
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 22.53
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 28,
      "name": "nickrosh/Evol-Instruct-Code-80k-v1",
      "domain": "code",
      "general_avg": 56.32,
      "math_avg": 44.84,
      "code_avg": 50.63,
      "reasoning_avg": 16.9,
      "overall_avg": 42.17,
      "general_scores": [
        74.26,
        50.47,
        57.39,
        41.7371429,
        74.82,
        51.1875,
        56.61,
        42.4564286,
        74.81,
        52.885,
        56.95,
        42.315
      ],
      "math_scores": [
        76.35,
        54.98,
        56.2,
        23.28,
        10.0,
        77.71,
        54.5,
        57.4,
        22.83,
        16.67,
        73.39,
        55.54,
        57.0,
        23.37,
        13.33
      ],
      "code_scores": [
        77.44,
        71.21,
        12.9,
        41.34,
        45.48,
        33.9,
        68.0,
        67.95,
        75.61,
        71.98,
        12.54,
        40.08,
        46.15,
        31.53,
        67.95,
        78.05,
        69.65,
        12.54,
        40.08,
        43.44,
        38.31,
        67.84
      ],
      "reasoning_scores": [
        0.41326087,
        33.52,
        0.41228261,
        33.44,
        0.41228261,
        33.44,
        0.39076087,
        33.2
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.63
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 51.51
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 56.98
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.17
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 75.82
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 55.01
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 56.87
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 23.16
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 13.33
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.03
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 70.95
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 12.66
              },
              {
                "metric": "lcb_code_execution",
                "score": 40.5
              },
              {
                "metric": "lcb_test_output",
                "score": 45.02
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 34.58
              },
              {
                "metric": "hard instruct",
                "score": 67.94
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.41
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 33.4
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 29,
      "name": "TokenBender/code_instructions_122k_alpaca_style",
      "domain": "code",
      "general_avg": 54.09,
      "math_avg": 40.41,
      "code_avg": 46.76,
      "reasoning_avg": 15.1,
      "overall_avg": 39.09,
      "general_scores": [
        65.44,
        52.59,
        55.78,
        42.89,
        63.63,
        53.985,
        55.37,
        42.6692857,
        64.56,
        53.6375,
        55.57,
        42.9792857
      ],
      "math_scores": [
        81.12,
        46.72,
        48.6,
        18.41,
        6.67,
        82.34,
        47.38,
        47.0,
        18.77,
        6.67,
        77.26,
        46.42,
        49.4,
        19.44,
        10.0
      ],
      "code_scores": [
        74.39,
        67.32,
        2.15,
        42.17,
        39.59,
        36.61,
        67.06,
        73.78,
        68.48,
        1.79,
        42.17,
        43.21,
        30.51,
        67.12,
        73.17,
        66.54,
        3.23,
        40.92,
        41.86,
        32.88,
        66.96
      ],
      "reasoning_scores": [
        0.38369565,
        30.32,
        0.3825,
        30.08,
        0.38565217,
        29.04
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 64.54
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 53.4
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 55.57
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.85
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 80.24
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 46.84
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 48.33
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 18.87
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 7.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 73.78
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 67.45
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 2.39
              },
              {
                "metric": "lcb_code_execution",
                "score": 41.75
              },
              {
                "metric": "lcb_test_output",
                "score": 41.55
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 33.33
              },
              {
                "metric": "hard instruct",
                "score": 67.05
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 29.81
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 30,
      "name": "iamtarun/python_code_instructions_18k_alpaca",
      "domain": "code",
      "general_avg": 56.57,
      "math_avg": 46.13,
      "code_avg": 46.44,
      "reasoning_avg": 16.35,
      "overall_avg": 41.37,
      "general_scores": [
        75.1,
        50.66,
        58.01,
        42.9478571,
        74.71,
        50.6575,
        57.61,
        43.2478571,
        74.48,
        51.18,
        57.74,
        42.5057143
      ],
      "math_scores": [
        80.52,
        54.22,
        54.2,
        24.86,
        13.33,
        81.05,
        54.94,
        56.6,
        25.29,
        13.33,
        81.43,
        54.54,
        52.8,
        24.86,
        20.0
      ],
      "code_scores": [
        75.61,
        71.6,
        0.36,
        42.17,
        34.62,
        35.59,
        67.93,
        73.78,
        67.32,
        0.0,
        42.38,
        35.97,
        32.88,
        68.66,
        73.78,
        69.26,
        0.0,
        42.38,
        36.43,
        35.93,
        68.61
      ],
      "reasoning_scores": [
        0.38195652,
        32.08,
        0.38565217,
        32.32,
        0.38586957,
        32.56
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 74.76
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 50.83
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.79
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 42.9
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 81.0
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.57
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 54.53
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.0
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 15.55
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 74.39
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 69.39
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 0.12
              },
              {
                "metric": "lcb_code_execution",
                "score": 42.31
              },
              {
                "metric": "lcb_test_output",
                "score": 35.67
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 34.8
              },
              {
                "metric": "hard instruct",
                "score": 68.4
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 32.32
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    },
    {
      "id": 31,
      "name": "ajibawa-2023/Python-Code-23k-ShareGPT",
      "domain": "code",
      "general_avg": 57.46,
      "math_avg": 49.84,
      "code_avg": 46.37,
      "reasoning_avg": 19.09,
      "overall_avg": 43.19,
      "general_scores": [
        76.64,
        48.8025,
        57.65,
        46.33,
        76.22,
        50.9975,
        57.76,
        45.9364286,
        76.23,
        49.385,
        57.47,
        46.1164286
      ],
      "math_scores": [
        82.56,
        61.22,
        60.8,
        25.72,
        23.33,
        84.15,
        61.54,
        61.2,
        25.61,
        10.0,
        83.24,
        61.2,
        61.6,
        25.47,
        20.0
      ],
      "code_scores": [
        75.61,
        68.09,
        14.34,
        17.95,
        44.8,
        35.93,
        67.14,
        79.27,
        67.7,
        14.34,
        15.24,
        44.8,
        35.59,
        67.06,
        77.44,
        69.65,
        13.26,
        15.87,
        46.38,
        35.59,
        67.67
      ],
      "reasoning_scores": [
        0.3825,
        37.84,
        0.39032609,
        37.92,
        0.38032609,
        37.6
      ],
      "task_details": {
        "general_tasks": [
          {
            "task_name": "Drop",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 76.36
              }
            ]
          },
          {
            "task_name": "IFEval",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 49.73
              }
            ]
          },
          {
            "task_name": "AGIEval",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 57.63
              }
            ]
          },
          {
            "task_name": "mmlu pro",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 46.13
              }
            ]
          }
        ],
        "math_tasks": [
          {
            "task_name": "GSM8k",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 83.32
              }
            ]
          },
          {
            "task_name": "MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.32
              }
            ]
          },
          {
            "task_name": "MATH500",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 61.2
              }
            ]
          },
          {
            "task_name": "Omni-MATH",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 25.6
              }
            ]
          },
          {
            "task_name": "AIME2024",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 17.78
              }
            ]
          }
        ],
        "code_tasks": [
          {
            "task_name": "humaneval",
            "metrics": [
              {
                "metric": "pass@1",
                "score": 77.44
              }
            ]
          },
          {
            "task_name": "MBPP",
            "metrics": [
              {
                "metric": "score 3shot",
                "score": 68.48
              }
            ]
          },
          {
            "task_name": "LiveCodeBench",
            "metrics": [
              {
                "metric": "lcb_code_generation",
                "score": 13.98
              },
              {
                "metric": "lcb_code_execution",
                "score": 16.35
              },
              {
                "metric": "lcb_test_output",
                "score": 45.33
              }
            ]
          },
          {
            "task_name": "BigCodeBench",
            "metrics": [
              {
                "metric": "full instruct",
                "score": 35.7
              },
              {
                "metric": "hard instruct",
                "score": 67.29
              }
            ]
          }
        ],
        "reasoning_tasks": [
          {
            "task_name": "ARC-C",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0.38
              }
            ]
          },
          {
            "task_name": "BBH",
            "metrics": [
              {
                "metric": "naive_average",
                "score": 37.79
              }
            ]
          },
          {
            "task_name": "GPQA",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          },
          {
            "task_name": "CaLM",
            "metrics": [
              {
                "metric": "accuracy avg",
                "score": 0
              }
            ]
          },
          {
            "task_name": "KOR-Bench",
            "metrics": [
              {
                "metric": "accuracy",
                "score": 0
              }
            ]
          }
        ]
      }
    }
  ]
}